{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Induration Image Generation GAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SarahGraceMaclean/GAN-Induration-image-generation/blob/master/Induration_Image_Generation_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWc8HN_jXJ6-",
        "colab_type": "code",
        "outputId": "35d653c4-725e-42c8-8575-c0db993842a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "import os\n",
        "os.chdir(\"drive/My Drive/\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFLkUcJDXRFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the libraries and setting variables \n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "batchSize = 25 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwMLUqFOXY7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Get data, pre-process data, load data\n",
        "\n",
        "def new_data():\n",
        "    compose = transforms.Compose(\n",
        "        [transforms.Resize((64,64)),\n",
        "         transforms.ToTensor(),         \n",
        "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "    out_dir = '150 Positive'\n",
        "    return dset.ImageFolder(root=out_dir, transform=compose)\n",
        "  \n",
        "dataset = new_data()\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size = batchSize, shuffle = True) # We use dataLoader to get the images of the training set batch by batch. Check that batch size and number of samples/images are evenly dividable. If tehy are not, the last batch will have a smaller number of samples (the left over ones) Avoid this by: drop_last = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtDrmyp1fU3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initialise Weights\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu90ny1TfU1r",
        "colab_type": "code",
        "outputId": "e084fcc5-d536-4945-b193-10140c1bc8f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "source": [
        "#Generator \n",
        "\n",
        "class G(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(G, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias = False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias = False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.main(input)\n",
        "        return output\n",
        "\n",
        "netG = G()\n",
        "netG.apply(weights_init)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "G(\n",
              "  (main): Sequential(\n",
              "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
              "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): LeakyReLU(negative_slope=0.2, inplace)\n",
              "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): LeakyReLU(negative_slope=0.2, inplace)\n",
              "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (8): LeakyReLU(negative_slope=0.2, inplace)\n",
              "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (11): LeakyReLU(negative_slope=0.2, inplace)\n",
              "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (13): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3R9QOg1zKTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Discriminator\n",
        "\n",
        "class D(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(D, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 4, 2, 1, bias = False),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.Conv2d(64, 128, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.Conv2d(128, 256, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.Conv2d(256, 512, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.Conv2d(512, 1, 4, 1, 0, bias = False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.main(input)\n",
        "        return output.view(-1)\n",
        "\n",
        "netD = D()\n",
        "netD.apply(weights_init)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizerD = optim.Adam(netD.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr = 0.0002, betas = (0.5, 0.999))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI4CvWxsMs1Z",
        "colab_type": "code",
        "outputId": "0dc68376-c1d7-4cc9-c431-a4bdfde4d4d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "# train\n",
        "\n",
        "!mkdir results_final\n",
        "\n",
        "gen_loss_avg = []\n",
        "disc_loss_avg = []\n",
        "\n",
        "\n",
        "for epoch in range(300):\n",
        "    gen_loss_avg.append(0)\n",
        "    disc_loss_avg.append(0)\n",
        "    num_batches = 0\n",
        "  \n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        \n",
        "        netD.zero_grad()\n",
        "        \n",
        "        real, _ = data\n",
        "        input = Variable(real)\n",
        "        target = Variable(torch.ones(input.size()[0]))\n",
        "        output = netD(input)\n",
        "        errD_real = criterion(output, target)\n",
        "        \n",
        "        noise = Variable(torch.randn(input.size()[0], 100, 1, 1))\n",
        "        fake = netG(noise)\n",
        "        target = Variable(torch.zeros(input.size()[0]))\n",
        "        output = netD(fake.detach())\n",
        "        errD_fake = criterion(output, target)\n",
        "        \n",
        "        errD = errD_real + errD_fake\n",
        "        errD.backward()\n",
        "        optimizerD.step()\n",
        "\n",
        "        netG.zero_grad()\n",
        "        target = Variable(torch.ones(input.size()[0]))\n",
        "        output = netD(fake)\n",
        "        errG = criterion(output, target)\n",
        "        errG.backward()\n",
        "        optimizerG.step()\n",
        "        \n",
        "        gen_loss_avg[-1] += errG.item()\n",
        "        disc_loss_avg[-1] += errD.item()\n",
        "        num_batches += 1\n",
        "        \n",
        "    gen_loss_avg[-1] /= num_batches\n",
        "    disc_loss_avg[-1] /= num_batches\n",
        "\n",
        "    print('[%d] Loss_D: %.4f Loss_G: %.4f' % (epoch, errD.item(), errG.item()))       \n",
        "\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        vutils.save_image(real, '%s/real_samples(7 B25 E300 positive).png' % \"./results_final\", normalize = True)\n",
        "        fake = netG(noise)\n",
        "vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d(7 B25 E300 positive).png' % (\"./results_final\", epoch), normalize = True)\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(gen_loss_avg, label='generator')\n",
        "plt.plot(disc_loss_avg, label='discriminator')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘results_final’: File exists\n",
            "[0] Loss_D: 0.1291 Loss_G: 6.3270\n",
            "[1] Loss_D: 0.0539 Loss_G: 7.1055\n",
            "[2] Loss_D: 0.0564 Loss_G: 7.6032\n",
            "[3] Loss_D: 0.0352 Loss_G: 8.9720\n",
            "[4] Loss_D: 0.1621 Loss_G: 16.4143\n",
            "[5] Loss_D: 1.6209 Loss_G: 35.6313\n",
            "[6] Loss_D: 0.0000 Loss_G: 34.5213\n",
            "[7] Loss_D: 0.0004 Loss_G: 34.1453\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-af185ec24d88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0merrG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0merrG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0moptimizerG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmVrU11AgwBe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.ion()\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(gen_loss_avg[0:299], label='generator')\n",
        "plt.plot(disc_loss_avg[0:299], label='discriminator')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}