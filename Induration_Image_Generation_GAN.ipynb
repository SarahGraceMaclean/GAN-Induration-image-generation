{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Induration Image Generation GAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SarahGraceMaclean/GAN-Induration-image-generation/blob/master/Induration_Image_Generation_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWc8HN_jXJ6-",
        "colab_type": "code",
        "outputId": "a4f61646-2ca6-4c4b-a415-580d8c51f771",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "import os\n",
        "os.chdir(\"drive/My Drive/\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-0a174d18e5d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drive/My Drive/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/My Drive/'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFLkUcJDXRFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the libraries and setting variables \n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "batchSize = 25 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwMLUqFOXY7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Get data, pre-process data, load data\n",
        "\n",
        "def new_data():\n",
        "    compose = transforms.Compose(\n",
        "        [transforms.Resize((64,64)),\n",
        "         transforms.ToTensor(),         \n",
        "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "    out_dir = '150 Positive'\n",
        "    return dset.ImageFolder(root=out_dir, transform=compose)\n",
        "  \n",
        "dataset = new_data()\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size = batchSize, shuffle = True) # We use dataLoader to get the images of the training set batch by batch. Check that batch size and number of samples/images are evenly dividable. If tehy are not, the last batch will have a smaller number of samples (the left over ones) Avoid this by: drop_last = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtDrmyp1fU3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initialise Weights\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu90ny1TfU1r",
        "colab_type": "code",
        "outputId": "f5955ab2-a911-4938-cdfc-3afcf37c541b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "source": [
        "#Generator \n",
        "\n",
        "class G(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(G, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias = False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias = False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.main(input)\n",
        "        return output\n",
        "\n",
        "netG = G()\n",
        "netG.apply(weights_init)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "G(\n",
              "  (main): Sequential(\n",
              "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
              "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): LeakyReLU(negative_slope=0.2, inplace)\n",
              "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): LeakyReLU(negative_slope=0.2, inplace)\n",
              "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (8): LeakyReLU(negative_slope=0.2, inplace)\n",
              "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (11): LeakyReLU(negative_slope=0.2, inplace)\n",
              "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (13): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3R9QOg1zKTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Discriminator\n",
        "\n",
        "class D(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(D, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 4, 2, 1, bias = False),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.Conv2d(64, 128, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.Conv2d(128, 256, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.Conv2d(256, 512, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.Conv2d(512, 1, 4, 1, 0, bias = False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.main(input)\n",
        "        return output.view(-1)\n",
        "\n",
        "netD = D()\n",
        "netD.apply(weights_init)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizerD = optim.Adam(netD.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr = 0.0002, betas = (0.5, 0.999))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI4CvWxsMs1Z",
        "colab_type": "code",
        "outputId": "c39ed468-d4d7-4dd0-b59a-0a3d38040523",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# train\n",
        "\n",
        "!mkdir results_final\n",
        "\n",
        "gen_loss_avg = []\n",
        "disc_loss_avg = []\n",
        "\n",
        "\n",
        "for epoch in range(300):\n",
        "    gen_loss_avg.append(0)\n",
        "    disc_loss_avg.append(0)\n",
        "    num_batches = 0\n",
        "  \n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        \n",
        "        netD.zero_grad()\n",
        "        \n",
        "        real, _ = data\n",
        "        input = Variable(real)\n",
        "        target = Variable(torch.ones(input.size()[0]))\n",
        "        output = netD(input)\n",
        "        errD_real = criterion(output, target)\n",
        "        \n",
        "        noise = Variable(torch.randn(input.size()[0], 100, 1, 1))\n",
        "        fake = netG(noise)\n",
        "        target = Variable(torch.zeros(input.size()[0]))\n",
        "        output = netD(fake.detach())\n",
        "        errD_fake = criterion(output, target)\n",
        "        \n",
        "        errD = errD_real + errD_fake\n",
        "        errD.backward()\n",
        "        optimizerD.step()\n",
        "\n",
        "        netG.zero_grad()\n",
        "        target = Variable(torch.ones(input.size()[0]))\n",
        "        output = netD(fake)\n",
        "        errG = criterion(output, target)\n",
        "        errG.backward()\n",
        "        optimizerG.step()\n",
        "        \n",
        "        gen_loss_avg[-1] += errG.item()\n",
        "        disc_loss_avg[-1] += errD.item()\n",
        "        num_batches += 1\n",
        "        \n",
        "    gen_loss_avg[-1] /= num_batches\n",
        "    disc_loss_avg[-1] /= num_batches\n",
        "\n",
        "    print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch, 30, i, len(dataloader), errD.item(), errG.item()))       \n",
        "\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        vutils.save_image(real, '%s/real_samples(7 B25 E300 positive).png' % \"./results_final\", normalize = True)\n",
        "        fake = netG(noise)\n",
        "vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d(7 B25 E300 positive).png' % (\"./results_final\", epoch), normalize = True)\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(gen_loss_avg, label='generator')\n",
        "plt.plot(disc_loss_avg, label='discriminator')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘results_final’: File exists\n",
            "[0/30][5/6] Loss_D: 0.1766 Loss_G: 6.4043\n",
            "[1/30][5/6] Loss_D: 0.0688 Loss_G: 7.2831\n",
            "[2/30][5/6] Loss_D: 0.0451 Loss_G: 8.3568\n",
            "[3/30][5/6] Loss_D: 0.0839 Loss_G: 6.4408\n",
            "[4/30][5/6] Loss_D: 0.0177 Loss_G: 14.4864\n",
            "[5/30][5/6] Loss_D: 0.0214 Loss_G: 14.6050\n",
            "[6/30][5/6] Loss_D: 0.0005 Loss_G: 13.7684\n",
            "[7/30][5/6] Loss_D: 0.0696 Loss_G: 11.7411\n",
            "[8/30][5/6] Loss_D: 0.0296 Loss_G: 8.6283\n",
            "[9/30][5/6] Loss_D: 0.0115 Loss_G: 8.8113\n",
            "[10/30][5/6] Loss_D: 0.0007 Loss_G: 14.2782\n",
            "[11/30][5/6] Loss_D: 2.4529 Loss_G: 42.4104\n",
            "[12/30][5/6] Loss_D: 0.0000 Loss_G: 44.5141\n",
            "[13/30][5/6] Loss_D: 0.0000 Loss_G: 44.4739\n",
            "[14/30][5/6] Loss_D: 0.0000 Loss_G: 44.2903\n",
            "[15/30][5/6] Loss_D: 0.0000 Loss_G: 44.5048\n",
            "[16/30][5/6] Loss_D: 0.0000 Loss_G: 44.3937\n",
            "[17/30][5/6] Loss_D: 0.0000 Loss_G: 44.2306\n",
            "[18/30][5/6] Loss_D: 0.0000 Loss_G: 44.5246\n",
            "[19/30][5/6] Loss_D: 0.0000 Loss_G: 44.4595\n",
            "[20/30][5/6] Loss_D: 0.0000 Loss_G: 44.5252\n",
            "[21/30][5/6] Loss_D: 0.0000 Loss_G: 44.0204\n",
            "[22/30][5/6] Loss_D: 0.0000 Loss_G: 44.5792\n",
            "[23/30][5/6] Loss_D: 0.0000 Loss_G: 44.5545\n",
            "[24/30][5/6] Loss_D: 0.0000 Loss_G: 44.4997\n",
            "[25/30][5/6] Loss_D: 0.0000 Loss_G: 44.6087\n",
            "[26/30][5/6] Loss_D: 0.0000 Loss_G: 44.3999\n",
            "[27/30][5/6] Loss_D: 0.0000 Loss_G: 44.3459\n",
            "[28/30][5/6] Loss_D: 0.0000 Loss_G: 44.2908\n",
            "[29/30][5/6] Loss_D: 0.0000 Loss_G: 44.4426\n",
            "[30/30][5/6] Loss_D: 0.0000 Loss_G: 44.3774\n",
            "[31/30][5/6] Loss_D: 0.0000 Loss_G: 44.4138\n",
            "[32/30][5/6] Loss_D: 0.0000 Loss_G: 44.4324\n",
            "[33/30][5/6] Loss_D: 0.0000 Loss_G: 44.4609\n",
            "[34/30][5/6] Loss_D: 0.0000 Loss_G: 44.5088\n",
            "[35/30][5/6] Loss_D: 0.0000 Loss_G: 44.5393\n",
            "[36/30][5/6] Loss_D: 0.0000 Loss_G: 44.2619\n",
            "[37/30][5/6] Loss_D: 0.0000 Loss_G: 44.3260\n",
            "[38/30][5/6] Loss_D: 0.0000 Loss_G: 44.3715\n",
            "[39/30][5/6] Loss_D: 0.0000 Loss_G: 44.4138\n",
            "[40/30][5/6] Loss_D: 0.0000 Loss_G: 44.2069\n",
            "[41/30][5/6] Loss_D: 0.0000 Loss_G: 44.3578\n",
            "[42/30][5/6] Loss_D: 0.0000 Loss_G: 44.4825\n",
            "[43/30][5/6] Loss_D: 0.0000 Loss_G: 44.2891\n",
            "[44/30][5/6] Loss_D: 0.0000 Loss_G: 44.5049\n",
            "[45/30][5/6] Loss_D: 0.0000 Loss_G: 44.4453\n",
            "[46/30][5/6] Loss_D: 0.0000 Loss_G: 44.3026\n",
            "[47/30][5/6] Loss_D: 0.0000 Loss_G: 44.5687\n",
            "[48/30][5/6] Loss_D: 0.0000 Loss_G: 44.2535\n",
            "[49/30][5/6] Loss_D: 0.0000 Loss_G: 44.4321\n",
            "[50/30][5/6] Loss_D: 0.0000 Loss_G: 44.5028\n",
            "[51/30][5/6] Loss_D: 0.0000 Loss_G: 44.4913\n",
            "[52/30][5/6] Loss_D: 0.0000 Loss_G: 44.2319\n",
            "[53/30][5/6] Loss_D: 0.0000 Loss_G: 44.2094\n",
            "[54/30][5/6] Loss_D: 0.0000 Loss_G: 44.4154\n",
            "[55/30][5/6] Loss_D: 0.0000 Loss_G: 44.4261\n",
            "[56/30][5/6] Loss_D: 0.0000 Loss_G: 44.3646\n",
            "[57/30][5/6] Loss_D: 0.0000 Loss_G: 44.4897\n",
            "[58/30][5/6] Loss_D: 0.0000 Loss_G: 44.4066\n",
            "[59/30][5/6] Loss_D: 0.0000 Loss_G: 44.1562\n",
            "[60/30][5/6] Loss_D: 0.0000 Loss_G: 44.5826\n",
            "[61/30][5/6] Loss_D: 0.0000 Loss_G: 44.2217\n",
            "[62/30][5/6] Loss_D: 0.0000 Loss_G: 44.3951\n",
            "[63/30][5/6] Loss_D: 0.0000 Loss_G: 44.2896\n",
            "[64/30][5/6] Loss_D: 0.0000 Loss_G: 44.4734\n",
            "[65/30][5/6] Loss_D: 0.0000 Loss_G: 44.4617\n",
            "[66/30][5/6] Loss_D: 0.0000 Loss_G: 44.2509\n",
            "[67/30][5/6] Loss_D: 0.0000 Loss_G: 44.1570\n",
            "[68/30][5/6] Loss_D: 0.0000 Loss_G: 44.5546\n",
            "[69/30][5/6] Loss_D: 0.0000 Loss_G: 44.6473\n",
            "[70/30][5/6] Loss_D: 0.0000 Loss_G: 44.3854\n",
            "[71/30][5/6] Loss_D: 0.0000 Loss_G: 44.5127\n",
            "[72/30][5/6] Loss_D: 0.0000 Loss_G: 44.4314\n",
            "[73/30][5/6] Loss_D: 0.0000 Loss_G: 44.4943\n",
            "[74/30][5/6] Loss_D: 0.0000 Loss_G: 44.4554\n",
            "[75/30][5/6] Loss_D: 0.0000 Loss_G: 44.3222\n",
            "[76/30][5/6] Loss_D: 0.0000 Loss_G: 44.1811\n",
            "[77/30][5/6] Loss_D: 0.0000 Loss_G: 44.4245\n",
            "[78/30][5/6] Loss_D: 0.0000 Loss_G: 44.4004\n",
            "[79/30][5/6] Loss_D: 0.0000 Loss_G: 44.3317\n",
            "[80/30][5/6] Loss_D: 0.0000 Loss_G: 44.2428\n",
            "[81/30][5/6] Loss_D: 0.0000 Loss_G: 44.3313\n",
            "[82/30][5/6] Loss_D: 0.0000 Loss_G: 44.3390\n",
            "[83/30][5/6] Loss_D: 0.0000 Loss_G: 44.3578\n",
            "[84/30][5/6] Loss_D: 0.0000 Loss_G: 44.4318\n",
            "[85/30][5/6] Loss_D: 0.0000 Loss_G: 44.1892\n",
            "[86/30][5/6] Loss_D: 0.0000 Loss_G: 44.2237\n",
            "[87/30][5/6] Loss_D: 0.0000 Loss_G: 44.3127\n",
            "[88/30][5/6] Loss_D: 0.0000 Loss_G: 44.4348\n",
            "[89/30][5/6] Loss_D: 0.0000 Loss_G: 44.5439\n",
            "[90/30][5/6] Loss_D: 0.0000 Loss_G: 44.3559\n",
            "[91/30][5/6] Loss_D: 0.0000 Loss_G: 44.5657\n",
            "[92/30][5/6] Loss_D: 0.0000 Loss_G: 44.5017\n",
            "[93/30][5/6] Loss_D: 0.0000 Loss_G: 44.2257\n",
            "[94/30][5/6] Loss_D: 0.0000 Loss_G: 44.2861\n",
            "[95/30][5/6] Loss_D: 0.0000 Loss_G: 44.4509\n",
            "[96/30][5/6] Loss_D: 0.0000 Loss_G: 44.3446\n",
            "[97/30][5/6] Loss_D: 0.0000 Loss_G: 44.4699\n",
            "[98/30][5/6] Loss_D: 0.0000 Loss_G: 44.4296\n",
            "[99/30][5/6] Loss_D: 0.0000 Loss_G: 44.4041\n",
            "[100/30][5/6] Loss_D: 0.0000 Loss_G: 44.3958\n",
            "[101/30][5/6] Loss_D: 0.0000 Loss_G: 44.3682\n",
            "[102/30][5/6] Loss_D: 0.0000 Loss_G: 44.2293\n",
            "[103/30][5/6] Loss_D: 0.0000 Loss_G: 44.3612\n",
            "[104/30][5/6] Loss_D: 0.0000 Loss_G: 44.5349\n",
            "[105/30][5/6] Loss_D: 0.0000 Loss_G: 44.7135\n",
            "[106/30][5/6] Loss_D: 0.0000 Loss_G: 44.4055\n",
            "[107/30][5/6] Loss_D: 0.0000 Loss_G: 44.4190\n",
            "[108/30][5/6] Loss_D: 0.0000 Loss_G: 44.5475\n",
            "[109/30][5/6] Loss_D: 0.0000 Loss_G: 44.4052\n",
            "[110/30][5/6] Loss_D: 0.0000 Loss_G: 44.5020\n",
            "[111/30][5/6] Loss_D: 0.0000 Loss_G: 44.6170\n",
            "[112/30][5/6] Loss_D: 0.0000 Loss_G: 44.3671\n",
            "[113/30][5/6] Loss_D: 0.0000 Loss_G: 44.4389\n",
            "[114/30][5/6] Loss_D: 0.0000 Loss_G: 44.4664\n",
            "[115/30][5/6] Loss_D: 0.0000 Loss_G: 44.2601\n",
            "[116/30][5/6] Loss_D: 0.0000 Loss_G: 44.4524\n",
            "[117/30][5/6] Loss_D: 0.0000 Loss_G: 44.3882\n",
            "[118/30][5/6] Loss_D: 0.0000 Loss_G: 44.2976\n",
            "[119/30][5/6] Loss_D: 0.0000 Loss_G: 44.3745\n",
            "[120/30][5/6] Loss_D: 0.0000 Loss_G: 44.3154\n",
            "[121/30][5/6] Loss_D: 0.0000 Loss_G: 44.3421\n",
            "[122/30][5/6] Loss_D: 0.0000 Loss_G: 44.4845\n",
            "[123/30][5/6] Loss_D: 0.0000 Loss_G: 44.4453\n",
            "[124/30][5/6] Loss_D: 0.0000 Loss_G: 44.2917\n",
            "[125/30][5/6] Loss_D: 0.0000 Loss_G: 44.4773\n",
            "[126/30][5/6] Loss_D: 0.0000 Loss_G: 44.4314\n",
            "[127/30][5/6] Loss_D: 0.0000 Loss_G: 44.4215\n",
            "[128/30][5/6] Loss_D: 0.0000 Loss_G: 44.2963\n",
            "[129/30][5/6] Loss_D: 0.0000 Loss_G: 44.4581\n",
            "[130/30][5/6] Loss_D: 0.0000 Loss_G: 44.5314\n",
            "[131/30][5/6] Loss_D: 0.0000 Loss_G: 44.4846\n",
            "[132/30][5/6] Loss_D: 0.0000 Loss_G: 44.2255\n",
            "[133/30][5/6] Loss_D: 0.0000 Loss_G: 44.5955\n",
            "[134/30][5/6] Loss_D: 0.0000 Loss_G: 44.3539\n",
            "[135/30][5/6] Loss_D: 0.0000 Loss_G: 44.4639\n",
            "[136/30][5/6] Loss_D: 0.0000 Loss_G: 44.6560\n",
            "[137/30][5/6] Loss_D: 0.0000 Loss_G: 44.2605\n",
            "[138/30][5/6] Loss_D: 0.0000 Loss_G: 44.4431\n",
            "[139/30][5/6] Loss_D: 0.0000 Loss_G: 44.3629\n",
            "[140/30][5/6] Loss_D: 0.0000 Loss_G: 44.3351\n",
            "[141/30][5/6] Loss_D: 0.0000 Loss_G: 44.5104\n",
            "[142/30][5/6] Loss_D: 0.0000 Loss_G: 44.4299\n",
            "[143/30][5/6] Loss_D: 0.0000 Loss_G: 44.2509\n",
            "[144/30][5/6] Loss_D: 0.0000 Loss_G: 44.4833\n",
            "[145/30][5/6] Loss_D: 0.0000 Loss_G: 44.4086\n",
            "[146/30][5/6] Loss_D: 0.0000 Loss_G: 44.4897\n",
            "[147/30][5/6] Loss_D: 0.0000 Loss_G: 44.3787\n",
            "[148/30][5/6] Loss_D: 0.0000 Loss_G: 44.3628\n",
            "[149/30][5/6] Loss_D: 0.0000 Loss_G: 44.3710\n",
            "[150/30][5/6] Loss_D: 0.0000 Loss_G: 44.3471\n",
            "[151/30][5/6] Loss_D: 0.0000 Loss_G: 44.4045\n",
            "[152/30][5/6] Loss_D: 0.0000 Loss_G: 44.2847\n",
            "[153/30][5/6] Loss_D: 0.0000 Loss_G: 44.4519\n",
            "[154/30][5/6] Loss_D: 0.0000 Loss_G: 44.4023\n",
            "[155/30][5/6] Loss_D: 0.0000 Loss_G: 44.4952\n",
            "[156/30][5/6] Loss_D: 0.0000 Loss_G: 44.2285\n",
            "[157/30][5/6] Loss_D: 0.0000 Loss_G: 44.3485\n",
            "[158/30][5/6] Loss_D: 0.0000 Loss_G: 44.6061\n",
            "[159/30][5/6] Loss_D: 0.0000 Loss_G: 44.4199\n",
            "[160/30][5/6] Loss_D: 0.0000 Loss_G: 44.4205\n",
            "[161/30][5/6] Loss_D: 0.0000 Loss_G: 44.4326\n",
            "[162/30][5/6] Loss_D: 0.0000 Loss_G: 44.3154\n",
            "[163/30][5/6] Loss_D: 0.0000 Loss_G: 44.3894\n",
            "[164/30][5/6] Loss_D: 0.0000 Loss_G: 44.2890\n",
            "[165/30][5/6] Loss_D: 0.0000 Loss_G: 44.3809\n",
            "[166/30][5/6] Loss_D: 0.0000 Loss_G: 44.4822\n",
            "[167/30][5/6] Loss_D: 0.0000 Loss_G: 44.5331\n",
            "[168/30][5/6] Loss_D: 0.0000 Loss_G: 44.3520\n",
            "[169/30][5/6] Loss_D: 0.0000 Loss_G: 44.3953\n",
            "[170/30][5/6] Loss_D: 0.0000 Loss_G: 44.2978\n",
            "[171/30][5/6] Loss_D: 0.0000 Loss_G: 44.2964\n",
            "[172/30][5/6] Loss_D: 0.0000 Loss_G: 44.3358\n",
            "[173/30][5/6] Loss_D: 0.0000 Loss_G: 44.3391\n",
            "[174/30][5/6] Loss_D: 0.0000 Loss_G: 44.1482\n",
            "[175/30][5/6] Loss_D: 0.0000 Loss_G: 44.3214\n",
            "[176/30][5/6] Loss_D: 0.0000 Loss_G: 44.4787\n",
            "[177/30][5/6] Loss_D: 0.0000 Loss_G: 44.3400\n",
            "[178/30][5/6] Loss_D: 0.0000 Loss_G: 44.4287\n",
            "[179/30][5/6] Loss_D: 0.0000 Loss_G: 44.4959\n",
            "[180/30][5/6] Loss_D: 0.0000 Loss_G: 44.4115\n",
            "[181/30][5/6] Loss_D: 0.0000 Loss_G: 44.3734\n",
            "[182/30][5/6] Loss_D: 0.0000 Loss_G: 44.3019\n",
            "[183/30][5/6] Loss_D: 0.0000 Loss_G: 44.2924\n",
            "[184/30][5/6] Loss_D: 0.0000 Loss_G: 44.2626\n",
            "[185/30][5/6] Loss_D: 0.0000 Loss_G: 44.3389\n",
            "[186/30][5/6] Loss_D: 0.0000 Loss_G: 44.5351\n",
            "[187/30][5/6] Loss_D: 0.0000 Loss_G: 44.4041\n",
            "[188/30][5/6] Loss_D: 0.0000 Loss_G: 44.2886\n",
            "[189/30][5/6] Loss_D: 0.0000 Loss_G: 44.3207\n",
            "[190/30][5/6] Loss_D: 0.0000 Loss_G: 44.5158\n",
            "[191/30][5/6] Loss_D: 0.0000 Loss_G: 44.3511\n",
            "[192/30][5/6] Loss_D: 0.0000 Loss_G: 44.3831\n",
            "[193/30][5/6] Loss_D: 0.0000 Loss_G: 44.4516\n",
            "[194/30][5/6] Loss_D: 0.0000 Loss_G: 44.4706\n",
            "[195/30][5/6] Loss_D: 0.0000 Loss_G: 44.3983\n",
            "[196/30][5/6] Loss_D: 0.0000 Loss_G: 44.5085\n",
            "[197/30][5/6] Loss_D: 0.0000 Loss_G: 44.4603\n",
            "[198/30][5/6] Loss_D: 0.0000 Loss_G: 44.0775\n",
            "[199/30][5/6] Loss_D: 0.0000 Loss_G: 44.2846\n",
            "[200/30][5/6] Loss_D: 0.0000 Loss_G: 44.4297\n",
            "[201/30][5/6] Loss_D: 0.0000 Loss_G: 44.3676\n",
            "[202/30][5/6] Loss_D: 0.0000 Loss_G: 44.5017\n",
            "[203/30][5/6] Loss_D: 0.0000 Loss_G: 44.3313\n",
            "[204/30][5/6] Loss_D: 0.0000 Loss_G: 44.5361\n",
            "[205/30][5/6] Loss_D: 0.0000 Loss_G: 44.2712\n",
            "[206/30][5/6] Loss_D: 0.0000 Loss_G: 44.5792\n",
            "[207/30][5/6] Loss_D: 0.0000 Loss_G: 44.4951\n",
            "[208/30][5/6] Loss_D: 0.0000 Loss_G: 44.4313\n",
            "[209/30][5/6] Loss_D: 0.0000 Loss_G: 44.3163\n",
            "[210/30][5/6] Loss_D: 0.0000 Loss_G: 44.2826\n",
            "[211/30][5/6] Loss_D: 0.0000 Loss_G: 44.3089\n",
            "[212/30][5/6] Loss_D: 0.0000 Loss_G: 44.4202\n",
            "[213/30][5/6] Loss_D: 0.0000 Loss_G: 44.2877\n",
            "[214/30][5/6] Loss_D: 0.0000 Loss_G: 44.4561\n",
            "[215/30][5/6] Loss_D: 0.0000 Loss_G: 44.4858\n",
            "[216/30][5/6] Loss_D: 0.0000 Loss_G: 44.3609\n",
            "[217/30][5/6] Loss_D: 0.0000 Loss_G: 44.2292\n",
            "[218/30][5/6] Loss_D: 0.0000 Loss_G: 44.3146\n",
            "[219/30][5/6] Loss_D: 0.0000 Loss_G: 44.3749\n",
            "[220/30][5/6] Loss_D: 0.0000 Loss_G: 44.2766\n",
            "[221/30][5/6] Loss_D: 0.0000 Loss_G: 44.4097\n",
            "[222/30][5/6] Loss_D: 0.0000 Loss_G: 44.4034\n",
            "[223/30][5/6] Loss_D: 0.0000 Loss_G: 44.2853\n",
            "[224/30][5/6] Loss_D: 0.0000 Loss_G: 44.2029\n",
            "[225/30][5/6] Loss_D: 0.0000 Loss_G: 44.3591\n",
            "[226/30][5/6] Loss_D: 0.0000 Loss_G: 44.1471\n",
            "[227/30][5/6] Loss_D: 0.0000 Loss_G: 44.4673\n",
            "[228/30][5/6] Loss_D: 0.0000 Loss_G: 44.4573\n",
            "[229/30][5/6] Loss_D: 0.0000 Loss_G: 44.4601\n",
            "[230/30][5/6] Loss_D: 0.0000 Loss_G: 44.4080\n",
            "[231/30][5/6] Loss_D: 0.0000 Loss_G: 44.2846\n",
            "[232/30][5/6] Loss_D: 0.0000 Loss_G: 44.2104\n",
            "[233/30][5/6] Loss_D: 0.0000 Loss_G: 44.3676\n",
            "[234/30][5/6] Loss_D: 0.0000 Loss_G: 44.2431\n",
            "[235/30][5/6] Loss_D: 0.0000 Loss_G: 44.4004\n",
            "[236/30][5/6] Loss_D: 0.0000 Loss_G: 44.4611\n",
            "[237/30][5/6] Loss_D: 0.0000 Loss_G: 44.4263\n",
            "[238/30][5/6] Loss_D: 0.0000 Loss_G: 44.4433\n",
            "[239/30][5/6] Loss_D: 0.0000 Loss_G: 44.4789\n",
            "[240/30][5/6] Loss_D: 0.0000 Loss_G: 44.4521\n",
            "[241/30][5/6] Loss_D: 0.0000 Loss_G: 44.2719\n",
            "[242/30][5/6] Loss_D: 0.0000 Loss_G: 44.4839\n",
            "[243/30][5/6] Loss_D: 0.0000 Loss_G: 44.5211\n",
            "[244/30][5/6] Loss_D: 0.0000 Loss_G: 44.4069\n",
            "[245/30][5/6] Loss_D: 0.0000 Loss_G: 44.3781\n",
            "[246/30][5/6] Loss_D: 0.0000 Loss_G: 44.3016\n",
            "[247/30][5/6] Loss_D: 0.0000 Loss_G: 44.3558\n",
            "[248/30][5/6] Loss_D: 0.0000 Loss_G: 44.3713\n",
            "[249/30][5/6] Loss_D: 0.0000 Loss_G: 44.4476\n",
            "[250/30][5/6] Loss_D: 0.0000 Loss_G: 44.2765\n",
            "[251/30][5/6] Loss_D: 0.0000 Loss_G: 44.3997\n",
            "[252/30][5/6] Loss_D: 0.0000 Loss_G: 44.2057\n",
            "[253/30][5/6] Loss_D: 0.0000 Loss_G: 44.4967\n",
            "[254/30][5/6] Loss_D: 0.0000 Loss_G: 44.4105\n",
            "[255/30][5/6] Loss_D: 0.0000 Loss_G: 44.3775\n",
            "[256/30][5/6] Loss_D: 0.0000 Loss_G: 44.3949\n",
            "[257/30][5/6] Loss_D: 0.0000 Loss_G: 44.1474\n",
            "[258/30][5/6] Loss_D: 0.0000 Loss_G: 44.6793\n",
            "[259/30][5/6] Loss_D: 0.0000 Loss_G: 44.2928\n",
            "[260/30][5/6] Loss_D: 0.0000 Loss_G: 44.4052\n",
            "[261/30][5/6] Loss_D: 0.0000 Loss_G: 44.3570\n",
            "[262/30][5/6] Loss_D: 0.0000 Loss_G: 44.4867\n",
            "[263/30][5/6] Loss_D: 0.0000 Loss_G: 44.3297\n",
            "[264/30][5/6] Loss_D: 0.0000 Loss_G: 44.4210\n",
            "[265/30][5/6] Loss_D: 0.0000 Loss_G: 44.2571\n",
            "[266/30][5/6] Loss_D: 0.0000 Loss_G: 44.3519\n",
            "[267/30][5/6] Loss_D: 0.0000 Loss_G: 44.0456\n",
            "[268/30][5/6] Loss_D: 0.0000 Loss_G: 44.4364\n",
            "[269/30][5/6] Loss_D: 0.0000 Loss_G: 44.4156\n",
            "[270/30][5/6] Loss_D: 0.0000 Loss_G: 44.3129\n",
            "[271/30][5/6] Loss_D: 0.0000 Loss_G: 44.2101\n",
            "[272/30][5/6] Loss_D: 0.0000 Loss_G: 44.1565\n",
            "[273/30][5/6] Loss_D: 0.0000 Loss_G: 44.3365\n",
            "[274/30][5/6] Loss_D: 0.0000 Loss_G: 44.3605\n",
            "[275/30][5/6] Loss_D: 0.0000 Loss_G: 44.4497\n",
            "[276/30][5/6] Loss_D: 0.0000 Loss_G: 44.1153\n",
            "[277/30][5/6] Loss_D: 0.0000 Loss_G: 44.4392\n",
            "[278/30][5/6] Loss_D: 0.0000 Loss_G: 44.2913\n",
            "[279/30][5/6] Loss_D: 0.0000 Loss_G: 44.3865\n",
            "[280/30][5/6] Loss_D: 0.0000 Loss_G: 44.1032\n",
            "[281/30][5/6] Loss_D: 0.0000 Loss_G: 44.4076\n",
            "[282/30][5/6] Loss_D: 0.0000 Loss_G: 44.2555\n",
            "[283/30][5/6] Loss_D: 0.0000 Loss_G: 44.2225\n",
            "[284/30][5/6] Loss_D: 0.0000 Loss_G: 44.3778\n",
            "[285/30][5/6] Loss_D: 0.0000 Loss_G: 44.2699\n",
            "[286/30][5/6] Loss_D: 0.0000 Loss_G: 44.2861\n",
            "[287/30][5/6] Loss_D: 0.0000 Loss_G: 44.5059\n",
            "[288/30][5/6] Loss_D: 0.0000 Loss_G: 44.2830\n",
            "[289/30][5/6] Loss_D: 0.0000 Loss_G: 44.4520\n",
            "[290/30][5/6] Loss_D: 0.0000 Loss_G: 44.4393\n",
            "[291/30][5/6] Loss_D: 0.0000 Loss_G: 44.3221\n",
            "[292/30][5/6] Loss_D: 0.0000 Loss_G: 44.4239\n",
            "[293/30][5/6] Loss_D: 0.0000 Loss_G: 44.3620\n",
            "[294/30][5/6] Loss_D: 0.0000 Loss_G: 44.4456\n",
            "[295/30][5/6] Loss_D: 0.0000 Loss_G: 44.2271\n",
            "[296/30][5/6] Loss_D: 0.0000 Loss_G: 44.2913\n",
            "[297/30][5/6] Loss_D: 0.0000 Loss_G: 44.4377\n",
            "[298/30][5/6] Loss_D: 0.0000 Loss_G: 44.4074\n",
            "[299/30][5/6] Loss_D: 0.0000 Loss_G: 44.6444\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmVrU11AgwBe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.ion()\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(gen_loss_avg[0:299], label='generator')\n",
        "plt.plot(disc_loss_avg[0:299], label='discriminator')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}