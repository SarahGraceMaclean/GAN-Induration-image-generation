{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Induration Image Generation GAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SarahGraceMaclean/GAN-Induration-image-generation/blob/master/Induration_Image_Generation_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skLGo4983IEj",
        "colab_type": "text"
      },
      "source": [
        "Batchsize changed from 64 to 200, then 25\n",
        "Epochs changes from 25 to 30, then 70"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWc8HN_jXJ6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "import os\n",
        "os.chdir(\"drive/My Drive/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFLkUcJDXRFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the libraries and setting variables \n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "batchSize = 25 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwMLUqFOXY7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Get data, pre-process data, load data\n",
        "\n",
        "def new_data():\n",
        "    compose = transforms.Compose(\n",
        "        [transforms.Resize((64,64)),\n",
        "         transforms.ToTensor(),         \n",
        "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "    out_dir = 'Induration Boxes Cropped'\n",
        "    return dset.ImageFolder(root=out_dir, transform=compose)\n",
        "  \n",
        "dataset = new_data()\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size = batchSize, shuffle = True) # We use dataLoader to get the images of the training set batch by batch. Check that batch size and number of samples/images are evenly dividable. If tehy are not, the last batch will have a smaller number of samples (the left over ones) Avoid this by: drop_last = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtDrmyp1fU3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#what is this\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu90ny1TfU1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Generator \n",
        "\n",
        "class G(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(G, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias = False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias = False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.main(input)\n",
        "        return output\n",
        "\n",
        "netG = G()\n",
        "netG.apply(weights_init)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3R9QOg1zKTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Discriminator\n",
        "\n",
        "class D(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(D, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 4, 2, 1, bias = False),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.Conv2d(64, 128, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.Conv2d(128, 256, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.Conv2d(256, 512, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.Conv2d(512, 1, 4, 1, 0, bias = False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.main(input)\n",
        "        return output.view(-1)\n",
        "\n",
        "netD = D()\n",
        "netD.apply(weights_init)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizerD = optim.Adam(netD.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr = 0.0002, betas = (0.5, 0.999))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI4CvWxsMs1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train\n",
        "\n",
        "!mkdir results\n",
        "\n",
        "gen_loss_avg = []\n",
        "disc_loss_avg = []\n",
        "\n",
        "\n",
        "for epoch in range(300):\n",
        "    gen_loss_avg.append(0)\n",
        "    disc_loss_avg.append(0)\n",
        "    num_batches = 0\n",
        "  \n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        \n",
        "        netD.zero_grad()\n",
        "        \n",
        "        real, _ = data\n",
        "        input = Variable(real)\n",
        "        target = Variable(torch.ones(input.size()[0]))\n",
        "        output = netD(input)\n",
        "        errD_real = criterion(output, target)\n",
        "        \n",
        "        noise = Variable(torch.randn(input.size()[0], 100, 1, 1))\n",
        "        fake = netG(noise)\n",
        "        target = Variable(torch.zeros(input.size()[0]))\n",
        "        output = netD(fake.detach())\n",
        "        errD_fake = criterion(output, target)\n",
        "        \n",
        "        errD = errD_real + errD_fake\n",
        "        errD.backward()\n",
        "        optimizerD.step()\n",
        "\n",
        "        netG.zero_grad()\n",
        "        target = Variable(torch.ones(input.size()[0]))\n",
        "        output = netD(fake)\n",
        "        errG = criterion(output, target)\n",
        "        errG.backward()\n",
        "        optimizerG.step()\n",
        "        \n",
        "        gen_loss_avg[-1] += errG.item()\n",
        "        disc_loss_avg[-1] += errD.item()\n",
        "        num_batches += 1\n",
        "        \n",
        "    gen_loss_avg[-1] /= num_batches\n",
        "    disc_loss_avg[-1] /= num_batches\n",
        "\n",
        "    print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch, 30, i, len(dataloader), errD.item(), errG.item()))       \n",
        "\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        vutils.save_image(real, '%s/real_samples(inception test B25 E150).png' % \"./results\", normalize = True)\n",
        "        fake = netG(noise)\n",
        "vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d(inception test B25 E150).png' % (\"./results\", epoch), normalize = True)\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(gen_loss_avg, label='generator')\n",
        "plt.plot(disc_loss_avg, label='discriminator')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmVrU11AgwBe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.ion()\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(gen_loss_avg[0:200], label='generator')\n",
        "plt.plot(disc_loss_avg[0:200], label='discriminator')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hph7gwwpEPt3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install image_slicer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqlSGIf28rBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cd results \n",
        "!pwd\n",
        "\n",
        "import image_slicer\n",
        "image_slicer.slice('test.png', 32)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}